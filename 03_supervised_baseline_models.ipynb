{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Supervised Learning - Baseline Models\n",
    "\n",
    "## Overview\n",
    "This notebook covers fundamental supervised learning algorithms for both classification and regression tasks.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Decision Trees\n",
    "4. K-Nearest Neighbors (KNN)\n",
    "5. Naive Bayes\n",
    "6. Support Vector Machines (SVM)\n",
    "7. Model Evaluation Metrics\n",
    "\n",
    "## Interview Focus:\n",
    "- Understanding of algorithm assumptions\n",
    "- When to use each algorithm\n",
    "- Bias-variance tradeoff\n",
    "- Evaluation metrics interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Coefficient: {lr.coef_[0]:.2f}\")\n",
    "print(f\"Intercept: {lr.intercept_:.2f}\")\n",
    "print(f\"RÂ² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, alpha=0.6, label='Training data')\n",
    "plt.scatter(X_test, y_test, alpha=0.6, color='orange', label='Test data')\n",
    "plt.plot(X, lr.predict(X), color='red', linewidth=2, label='Regression line')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic classification data\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary and ROC curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[0].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "axes[0].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, \n",
    "               cmap='RdYlBu', edgecolors='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('Logistic Regression Decision Boundary')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "            label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Train decision tree\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_dt):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\nFeature Importances: {dt.feature_importances_}\")\n",
    "\n",
    "# Visualize tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt, filled=True, feature_names=['Feature 1', 'Feature 2'], \n",
    "         class_names=['Class 0', 'Class 1'], fontsize=10)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Find optimal K\n",
    "k_range = range(1, 21)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    scores.append(knn.score(X_test_scaled, y_test))\n",
    "\n",
    "# Plot K vs Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, scores, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN: Finding Optimal K')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train with optimal K\n",
    "optimal_k = k_range[np.argmax(scores)]\n",
    "print(f\"Optimal K: {optimal_k}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\nKNN Results (K={optimal_k}):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_knn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb = nb.predict(X_test_scaled)\n",
    "\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_nb):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM with different kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "svm_results = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred_svm = svm.predict(X_test_scaled)\n",
    "    svm_results[kernel] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "        'f1': f1_score(y_test, y_pred_svm)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"SVM Results with Different Kernels:\")\n",
    "for kernel, metrics in svm_results.items():\n",
    "    print(f\"\\n{kernel.upper()} Kernel:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "# Visualize SVM decision boundary (RBF kernel)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, \n",
    "           cmap='RdYlBu', edgecolors='black', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM (RBF Kernel) Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=optimal_k),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "results_df.plot(x='Model', y=['Accuracy', 'Precision', 'Recall', 'F1-Score'], \n",
    "               kind='bar', ax=axes[0], rot=45)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Metrics')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Cross-validation comparison\n",
    "axes[1].bar(results_df['Model'], results_df['CV Mean'], \n",
    "           yerr=results_df['CV Std'], capsize=5, alpha=0.7)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('CV Score')\n",
    "axes[1].set_title('Cross-Validation Scores (with std)')\n",
    "axes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n",
    "\n",
    "### Q1: What assumptions does Linear Regression make?\n",
    "**Answer:**\n",
    "1. **Linearity**: Linear relationship between features and target\n",
    "2. **Independence**: Observations are independent\n",
    "3. **Homoscedasticity**: Constant variance of errors\n",
    "4. **Normality**: Errors are normally distributed\n",
    "5. **No multicollinearity**: Features are not highly correlated\n",
    "\n",
    "### Q2: When to use Logistic Regression vs Decision Trees?\n",
    "**Answer:**\n",
    "- **Logistic Regression**: When relationship is linear, need probability estimates, interpretability is important\n",
    "- **Decision Trees**: When relationships are non-linear, feature interactions are important, interpretability with complex patterns\n",
    "\n",
    "### Q3: What is the bias-variance tradeoff?\n",
    "**Answer:**\n",
    "- **Bias**: Error from wrong assumptions (underfitting)\n",
    "- **Variance**: Error from sensitivity to training data (overfitting)\n",
    "- Goal: Balance both to minimize total error\n",
    "- Simple models: High bias, low variance\n",
    "- Complex models: Low bias, high variance\n",
    "\n",
    "### Q4: Why normalize/standardize features for KNN and SVM?\n",
    "**Answer:** These algorithms use distance metrics. Features with larger scales dominate the distance calculation, leading to biased predictions. Normalization ensures all features contribute equally.\n",
    "\n",
    "### Q5: What's the difference between precision and recall?\n",
    "**Answer:**\n",
    "- **Precision**: Of predicted positives, how many are actually positive? (TP / (TP + FP))\n",
    "- **Recall**: Of actual positives, how many did we predict? (TP / (TP + FN))\n",
    "- Use precision when false positives are costly\n",
    "- Use recall when false negatives are costly\n",
    "\n",
    "### Q6: What is overfitting and how to prevent it?\n",
    "**Answer:**\n",
    "- **Overfitting**: Model learns training data too well, including noise\n",
    "- **Prevention**:\n",
    "  - Cross-validation\n",
    "  - Regularization (L1, L2)\n",
    "  - Pruning (decision trees)\n",
    "  - More training data\n",
    "  - Simpler models\n",
    "  - Early stopping\n",
    "\n",
    "### Q7: Explain ROC curve and AUC\n",
    "**Answer:**\n",
    "- **ROC**: Plots True Positive Rate vs False Positive Rate at various thresholds\n",
    "- **AUC**: Area Under the ROC Curve (0 to 1)\n",
    "  - AUC = 1: Perfect classifier\n",
    "  - AUC = 0.5: Random classifier\n",
    "  - Higher AUC = Better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Implement linear regression from scratch using gradient descent\n",
    "2. Compare model performance on imbalanced datasets\n",
    "3. Perform hyperparameter tuning for each model\n",
    "4. Analyze feature importance across different models\n",
    "5. Handle missing values and evaluate impact on model performance\n",
    "6. Implement stratified k-fold cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
