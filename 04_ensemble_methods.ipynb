{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Ensemble Methods\n\n",
    "## Overview\n",
    "This notebook covers advanced ensemble learning techniques that combine multiple models for better predictions.\n\n",
    "## Topics Covered:\n",
    "1. Bagging and Random Forest\n2. Boosting (AdaBoost, Gradient Boosting)\n",
    "3. XGBoost\n4. LightGBM\n5. Stacking and Blending\n6. Voting Classifiers\n\n",
    "## Interview Focus:\n",
    "- Understanding ensemble principles\n- Differences between bagging and boosting\n",
    "- When to use each ensemble method\n",
    "- Handling overfitting in ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                          n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n\n",
    "print('Random Forest Results:')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}')\n",
    "print(f'Cross-validation score: {cross_val_score(rf, X_train, y_train, cv=5).mean():.4f}')\n\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': [f'Feature_{i}' for i in range(20)],\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), feature_importance['importance'].head(10))\n",
    "plt.yticks(range(10), feature_importance['feature'].head(10))\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n\n",
    "print('AdaBoost Results:')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_ada):.4f}')\n",
    "print(f'Cross-validation score: {cross_val_score(ada, X_train, y_train, cv=5).mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                               max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n\n",
    "print('Gradient Boosting Results:')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}')\n",
    "print(f'Cross-validation score: {cross_val_score(gb, X_train, y_train, cv=5).mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Install xgboost if not available: pip install xgboost\n",
    "try:\n",
    "    import xgboost as xgb\n    \n",
    "    # XGBoost\n",
    "    xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                               max_depth=3, random_state=42, eval_metric='logloss')\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb_clf.predict(X_test)\n    \n",
    "    print('XGBoost Results:')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}')\n",
    "    print(f'Cross-validation score: {cross_val_score(xgb_clf, X_train, y_train, cv=5).mean():.4f}')\n",
    "except ImportError:\n",
    "    print('XGBoost not installed. Install with: pip install xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n\n",
    "# Create individual classifiers\n",
    "clf1 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "clf3 = SVC(kernel='rbf', probability=True, random_state=42)\n\n",
    "# Hard voting\n",
    "voting_hard = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3)], \n",
    "                              voting='hard')\n",
    "voting_hard.fit(X_train, y_train)\n",
    "y_pred_voting = voting_hard.predict(X_test)\n\n",
    "print('Voting Classifier Results:')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_voting):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all ensemble methods\n",
    "models = {\n",
    "    'Random Forest': y_pred_rf,\n",
    "    'AdaBoost': y_pred_ada,\n",
    "    'Gradient Boosting': y_pred_gb,\n",
    "    'Voting Classifier': y_pred_voting\n",
    "}\n\n",
    "try:\n",
    "    models['XGBoost'] = y_pred_xgb\n",
    "except:\n",
    "    pass\n\n",
    "results = pd.DataFrame({\n",
    "    'Model': models.keys(),\n",
    "    'Accuracy': [accuracy_score(y_test, pred) for pred in models.values()]\n",
    "})\n\n",
    "print('\\nEnsemble Methods Comparison:')\n",
    "print(results.to_string(index=False))\n\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results['Model'], results['Accuracy'], alpha=0.7)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Ensemble Methods Performance Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n\n",
    "### Q1: What's the difference between bagging and boosting?\n",
    "**Answer:**\n",
    "- **Bagging**: Trains models in parallel on random subsets of data, reduces variance\n",
    "- **Boosting**: Trains models sequentially, each correcting previous errors, reduces bias\n",
    "- Bagging example: Random Forest\n",
    "- Boosting examples: AdaBoost, Gradient Boosting, XGBoost\n\n",
    "### Q2: How does Random Forest prevent overfitting?\n",
    "**Answer:**\n",
    "1. Bootstrap sampling (random subsets of data)\n",
    "2. Random feature selection at each split\n",
    "3. Averaging predictions from multiple trees\n",
    "4. Max depth and other regularization parameters\n\n",
    "### Q3: What makes XGBoost better than Gradient Boosting?\n",
    "**Answer:**\n",
    "- Regularization to prevent overfitting\n",
    "- Parallel processing for speed\n",
    "- Handling of missing values\n",
    "- Tree pruning using max_depth\n",
    "- Built-in cross-validation\n\n",
    "### Q4: When to use hard vs soft voting?\n",
    "**Answer:**\n",
    "- **Hard voting**: Majority vote (faster)\n",
    "- **Soft voting**: Average probabilities (better when models have different strengths)\n",
    "- Soft voting generally performs better but requires probability estimates\n\n",
    "## Practice Exercises\n",
    "1. Implement stacking ensemble with multiple layers\n",
    "2. Compare bagging with different base estimators\n",
    "3. Tune XGBoost hyperparameters using grid search\n",
    "4. Analyze feature importance across different ensemble methods\n",
    "5. Implement custom voting ensemble with weighted votes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}