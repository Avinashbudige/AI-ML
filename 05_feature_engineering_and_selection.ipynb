{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Feature Engineering and Selection\n\n",
    "## Overview\nThis notebook covers feature engineering and selection techniques essential for machine learning and deep learning.\n\n",
    "## Topics Covered:\n1. Feature Scaling\n2. Feature Encoding\n3. Feature Creation\n4. Feature Selection Methods\n5. Dimensionality Curse\n6. PCA for Feature Extraction\n\n",
    "## Interview Focus:\n- Understanding core concepts\n- Practical implementation\n- When to apply each technique\n- Common pitfalls and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n\nThis notebook covers:\n\n- Feature scaling (StandardScaler, MinMaxScaler)\n- Encoding categorical variables\n- Creating polynomial features\n- SelectKBest, RFE\n- Variance thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Examples\n\nThe following sections provide practical implementations of the concepts above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic setup and data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Test samples: {len(X_test)}')\n",
    "print(f'Number of features: {X.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'income': np.random.exponential(50000, 1000),\n",
    "    'age': np.random.randint(18, 80, 1000),\n",
    "    'credit_score': np.random.randint(300, 850, 1000)\n",
    "})\n\n",
    "print('Original data statistics:')\n",
    "print(data.describe())\n\n",
    "# Standard Scaler (z-score normalization)\n",
    "scaler_std = StandardScaler()\n",
    "data_std = pd.DataFrame(scaler_std.fit_transform(data), columns=data.columns)\n\n",
    "# Min-Max Scaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_minmax = pd.DataFrame(scaler_minmax.fit_transform(data), columns=data.columns)\n\n",
    "# Robust Scaler (resistant to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "data_robust = pd.DataFrame(scaler_robust.fit_transform(data), columns=data.columns)\n\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "data['income'].hist(ax=axes[0,0], bins=30, edgecolor='black')\n",
    "axes[0,0].set_title('Original Income')\n",
    "data_std['income'].hist(ax=axes[0,1], bins=30, edgecolor='black', color='green')\n",
    "axes[0,1].set_title('StandardScaler')\n",
    "data_minmax['income'].hist(ax=axes[1,0], bins=30, edgecolor='black', color='orange')\n",
    "axes[1,0].set_title('MinMaxScaler')\n",
    "data_robust['income'].hist(ax=axes[1,1], bins=30, edgecolor='black', color='red')\n",
    "axes[1,1].set_title('RobustScaler')\n",
    "plt.tight_layout()\n",
    "plt.show()\n\n",
    "print('\\nStandardScaler statistics:')\n",
    "print(data_std.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n\n",
    "# Sample categorical data\n",
    "df_cat = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue', 'green'],\n",
    "    'size': ['small', 'medium', 'large', 'medium', 'small', 'large'],\n",
    "    'price': [10, 20, 30, 15, 12, 35]\n",
    "})\n\n",
    "print('Original data:')\n",
    "print(df_cat)\n\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df_cat['color_label'] = le.fit_transform(df_cat['color'])\n",
    "print('\\nLabel Encoding:')\n",
    "print(df_cat[['color', 'color_label']])\n\n",
    "# One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df_cat, columns=['color', 'size'], prefix=['color', 'size'])\n",
    "print('\\nOne-Hot Encoding:')\n",
    "print(df_onehot.head())\n\n",
    "# Ordinal Encoding (for ordered categories)\n",
    "size_order = [['small', 'medium', 'large']]\n",
    "oe = OrdinalEncoder(categories=size_order)\n",
    "df_cat['size_ordinal'] = oe.fit_transform(df_cat[['size']])\n",
    "print('\\nOrdinal Encoding (size):')\n",
    "print(df_cat[['size', 'size_ordinal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=10, \n",
    "                          n_redundant=5, n_repeated=2, random_state=42)\n\n",
    "# Method 1: SelectKBest\n",
    "selector_kbest = SelectKBest(f_classif, k=10)\n",
    "X_kbest = selector_kbest.fit_transform(X, y)\n",
    "print('SelectKBest:')\n",
    "print(f'Original features: {X.shape[1]}')\n",
    "print(f'Selected features: {X_kbest.shape[1]}')\n",
    "print(f'Selected feature indices: {selector_kbest.get_support(indices=True)}')\n\n",
    "# Method 2: Recursive Feature Elimination (RFE)\n",
    "estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "X_rfe = selector_rfe.fit_transform(X, y)\n",
    "print('\\nRecursive Feature Elimination:')\n",
    "print(f'Selected features: {X_rfe.shape[1]}')\n",
    "print(f'Feature ranking: {selector_rfe.ranking_}')\n\n",
    "# Method 3: Variance Threshold\n",
    "selector_var = VarianceThreshold(threshold=0.1)\n",
    "X_var = selector_var.fit_transform(X)\n",
    "print('\\nVariance Threshold:')\n",
    "print(f'Features after variance threshold: {X_var.shape[1]}')\n\n",
    "# Feature importance from Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': [f'Feature_{i}' for i in range(X.shape[1])],\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(15), feature_importance['importance'].head(15))\n",
    "plt.yticks(range(15), feature_importance['feature'].head(15))\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Features by Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n\n",
    "# Polynomial features\n",
    "X_sample = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_sample)\n\n",
    "print('Original features:')\n",
    "print(X_sample)\n",
    "print('\\nPolynomial features (degree=2):')\n",
    "print(X_poly)\n",
    "print('\\nFeature names:')\n",
    "print(poly.get_feature_names_out(['x1', 'x2']))\n\n",
    "# Domain-specific feature engineering example\n",
    "df_dates = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=100),\n",
    "    'sales': np.random.randint(100, 1000, 100)\n",
    "})\n\n",
    "# Extract time-based features\n",
    "df_dates['year'] = df_dates['date'].dt.year\n",
    "df_dates['month'] = df_dates['date'].dt.month\n",
    "df_dates['day'] = df_dates['date'].dt.day\n",
    "df_dates['dayofweek'] = df_dates['date'].dt.dayofweek\n",
    "df_dates['is_weekend'] = df_dates['dayofweek'].isin([5, 6]).astype(int)\n\n",
    "print('\\nTime-based feature engineering:')\n",
    "print(df_dates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n\n1. Always split data before preprocessing\n2. Use cross-validation for model evaluation\n3. Monitor for overfitting/underfitting\n4. Document hyperparameters and experiments\n5. Start simple, then add complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n\n",
    "### Q1: What are the main concepts in Feature Engineering and Selection?\n",
    "**Answer:** Feature Scaling, Feature Encoding, Feature Creation\n\n",
    "### Q2: When should you apply these techniques?\n",
    "**Answer:** Apply when you need to improve model performance, reduce dimensionality, or extract meaningful patterns from data.\n\n",
    "### Q3: What are common challenges?\n",
    "**Answer:** Common challenges include overfitting, computational complexity, hyperparameter tuning, and interpretation of results.\n\n",
    "## Practice Exercises\n\n",
    "1. Implement the core algorithms from scratch\n",
    "2. Compare performance across different methods\n",
    "3. Tune hyperparameters systematically\n",
    "4. Apply to real-world datasets\n",
    "5. Analyze and interpret results\n\n",
    "## Additional Resources\n\n",
    "- Scikit-learn documentation\n",
    "- TensorFlow and PyTorch tutorials\n",
    "- Relevant research papers\n",
    "- Online courses and books"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}